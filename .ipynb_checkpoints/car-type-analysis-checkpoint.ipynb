{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:16:54.247972Z",
     "start_time": "2024-12-19T14:16:50.938882Z"
    }
   },
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq,AutoTokenizer,AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from d2l import torch as d2l\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:16:54.263972Z",
     "start_time": "2024-12-19T14:16:54.250974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install evaluate"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:16:54.327481Z",
     "start_time": "2024-12-19T14:16:54.312977Z"
    }
   },
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:16:55.402185Z",
     "start_time": "2024-12-19T14:16:54.344484Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = \"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\"\n",
    "login(token=hf_token)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:16:55.433827Z",
     "start_time": "2024-12-19T14:16:55.418827Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    # 获取可用的 GPU 设备数量\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"可用 GPU 数量:\", num_devices)\n",
    "\n",
    "    # 遍历所有可用的 GPU 设备并打印详细信息\n",
    "    for i in range(num_devices):\n",
    "        device = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i} 的详细信息:\")\n",
    "        print(\"名称:\", device.name)\n",
    "        print(\"计算能力:\", f\"{device.major}.{device.minor}\")\n",
    "        print(\"内存总量 (GB):\", round(device.total_memory / (1024**3), 1))\n",
    "else:\n",
    "    print(\"没有可用的 GPU\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用 GPU 数量: 1\n",
      "\n",
      "GPU 0 的详细信息:\n",
      "名称: NVIDIA GeForce RTX 4070 SUPER\n",
      "计算能力: 8.9\n",
      "内存总量 (GB): 12.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:16:56.922206Z",
     "start_time": "2024-12-19T14:16:55.450828Z"
    }
   },
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "F:\\anaconda\\envs\\d2l\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:01.419546Z",
     "start_time": "2024-12-19T14:16:56.939200Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"lamm-mit/MechanicsMaterials\")\n",
    "\n",
    "ds\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Administrator/.cache/huggingface/datasets/lamm-mit___csv/lamm-mit--MechanicsMaterials-0d433f0e6c9301bb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a37573d223674e02a875bc7bf4f8b615"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'question', 'answer'],\n",
       "        num_rows: 8204\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:01.451054Z",
     "start_time": "2024-12-19T14:17:01.436053Z"
    }
   },
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:03.932116Z",
     "start_time": "2024-12-19T14:17:01.468054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=500,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # 显式设置结束标志\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer\n",
    "\n",
    "\n",
    "print(inference('What are the structural characteristics and modes of action of antimicrobial peptides?', model, tokenizer))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\d2l\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "F:\\anaconda\\envs\\d2l\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The structure is a complex, multi-dimensional system consisting of three layers: (1) an outer layer that contains all known antibacterial molecules; (2), which contain only one or two antigens. The innermost membrane consists mainly composed by proteins called endoplasmic reticulars with their own internal structures such as helixes in each side cells where they form complexes to protect against bacterial infection but also act as reservoirs for other bacteria including viruses like Encephalomyelitis B virus [12]. In addition there are many different types used throughout the body so it's important not just those who use them on themselves because these can be harmful if taken too much orally when taking antibiotics[13]. Antibiotics have been shown at various stages during development before being applied directly into human skin.[14] However this has never happened since most people do take some type(or combination thereof) after using certain drugs while still having normal immune systems functioning normally without any problems due either to lack osmotic immunity from infections caused by pathogens present within our bodies,[15][16], nor does anyone know how long we've had exposure through oral ingestion prior - even though I'm sure you're familiar with what happens afterwards! It seems unlikely however given my experience over time about antibiotic resistance among humans...and especially considering recent studies showing no significant difference between men exposed to high doses versus women treated less than 10 days later compared towards similar levels across groups according both sexes' ages/sexings etc..I think its possible something more could happen here....but until then please don't let me down!!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:03.964114Z",
     "start_time": "2024-12-19T14:17:03.949117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inputs(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  \n",
    "\n",
    "  # Strip the prompt\n",
    "  # generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return input_ids.to(device)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:04.012129Z",
     "start_time": "2024-12-19T14:17:03.981115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(inputs('What are the structural characteristics and modes of action of antimicrobial peptides?', model, tokenizer))\n",
    "loss = outputs.loss\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:04.265465Z",
     "start_time": "2024-12-19T14:17:04.250466Z"
    }
   },
   "source": [
    "def tokenize_and_split_data(dataset, tokenizer):\n",
    "    dataset = splited_dataset(dataset, tokenizer)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def splited_dataset(dataset, tokenizer):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_dataset = dataset.map(\n",
    "        get_tokenize_function(tokenizer), # returns tokenize_function\n",
    "        batched=True,\n",
    "        # batch_size=10,\n",
    "        drop_last_batch=True,\n",
    "        # remove_columns=dataset.column_names\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "    split_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "    return split_dataset\n",
    "\n",
    "def get_tokenize_function(tokenizer, _max_length = 2048):\n",
    "\n",
    "  def tokenize_function(examples):\n",
    "    max_length = _max_length\n",
    "\n",
    "    # Set pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "        text = [q + a for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "        text = [i + o for i, o in zip(examples[\"input\"], examples[\"output\"])]\n",
    "    else:\n",
    "        text = examples[\"text\"]  # 假设直接包含 \"text\" 字段\n",
    "\n",
    "    # Run tokenizer on all the text (the input and the output)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "\n",
    "        # Return tensors in a numpy array (other options are pytorch or tf objects)\n",
    "        return_tensors=\"np\",\n",
    "\n",
    "        # Padding type is to pad to the longest sequence in the batch (other option is to a certain max length, or no padding)\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Calculate max length\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "    return tokenized_inputs\n",
    "  return tokenize_function"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:04.486480Z",
     "start_time": "2024-12-19T14:17:04.455480Z"
    }
   },
   "source": "train_dataset, test_dataset = tokenize_and_split_data(ds, tokenizer)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\lamm-mit___csv\\lamm-mit--MechanicsMaterials-0d433f0e6c9301bb\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-122c6b6ac2fcd0cc.arrow\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:04.692529Z",
     "start_time": "2024-12-19T14:17:04.677522Z"
    }
   },
   "source": [
    "train_dataset = train_dataset.remove_columns(['Unnamed: 0', 'question','answer'])\n",
    "test_dataset = test_dataset.remove_columns(['Unnamed: 0', 'question','answer'])"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:04.899106Z",
     "start_time": "2024-12-19T14:17:04.884107Z"
    }
   },
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4, collate_fn=data_collator)  # 通过这里的dataloader，每个batch的seq_len可能不同\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=data_collator)\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:05.105129Z",
     "start_time": "2024-12-19T14:17:05.090130Z"
    }
   },
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([4, 346]),\n",
       " 'attention_mask': torch.Size([4, 346]),\n",
       " 'labels': torch.Size([4, 346])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:05.311152Z",
     "start_time": "2024-12-19T14:17:05.297150Z"
    }
   },
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\d2l\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:05.515401Z",
     "start_time": "2024-12-19T14:17:05.500391Z"
    }
   },
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:19:05.844697100Z",
     "start_time": "2024-12-19T14:18:53.961036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_batches, timer = len(train_dataloader), d2l.Timer()\n",
    "\n",
    "animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=\"train_loss\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    i = 0\n",
    "    metric = d2l.Accumulator(1)\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        timer.start()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        if i % 10 ==0:     \n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        metric.add(loss)\n",
    "        timer.stop()\n",
    "        if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "            animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] , None))\n",
    "    measures = f'train loss {metric[0] :.3f}'\n",
    "print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c31af5ece7d45d5822493126f29c14e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.6875, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2734, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2969, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6055, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5898, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4883, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5957, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8730, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2988, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2344, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1250, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4121, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3477, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5859, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9277, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7852, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3379, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6621, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6797, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4629, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6270, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6621, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8105, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T14:17:06.047431500Z",
     "start_time": "2024-12-19T10:12:47.220606Z"
    }
   },
   "source": [
    "# import evaluate\n",
    "# \n",
    "# metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "# model.eval()\n",
    "# for batch in eval_dataloader:\n",
    "#     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**batch)\n",
    "# \n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=-1)\n",
    "#     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "# \n",
    "# metric.compute()"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:14:23.817249Z",
     "start_time": "2024-12-19T10:14:22.934611Z"
    }
   },
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=100,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # 显式设置结束标志\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer\n",
    "\n",
    "\n",
    "print(inference('What are the structural characteristics and modes of action of antimicrobial peptides?', model, tokenizer))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The structure, properties associated with these substances can be determined by various factors such as bacterial origin or type. These include their specific composition (eigenvalues), which is important for predicting antibacterial effects on bacteria in different environments like viruses/organisms that do not experience any immune response to infection without being infected). The presence between a group consisting solely from an antibiotic bacterium allows it access into cells' cell membranes through its own micro\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:12:48.882250Z",
     "start_time": "2024-12-19T10:12:48.867231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save_path = \"./my_model3\"\n",
    "# \n",
    "# # 保存模型和分词器到指定路径\n",
    "# model2.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# \n",
    "# print(f\"模型和分词器已保存到 {save_path}\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:12:49.419986Z",
     "start_time": "2024-12-19T10:12:49.404982Z"
    }
   },
   "source": [
    "# # 保存模型到指定路径\n",
    "# def save_model_and_tokenizer(model, tokenizer, save_path):\n",
    "#     # 将模型转移到 CPU 并转换为 FP32 格式\n",
    "#     model = model.to(\"cpu\").float()\n",
    "# \n",
    "#     # 保存模型和分词器\n",
    "#     model.save_pretrained(save_path)\n",
    "#     tokenizer.save_pretrained(save_path)\n",
    "# \n",
    "#     print(f\"模型和分词器已成功保存到 {save_path}\")\n",
    "# \n",
    "# # 定义保存路径\n",
    "# save_path = \"./my_model2\"\n",
    "# save_model_and_tokenizer(model, tokenizer, save_path)\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:12:49.944539Z",
     "start_time": "2024-12-19T10:12:49.929541Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6310467,
     "sourceId": 10210276,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
